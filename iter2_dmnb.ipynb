{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<style>\n",
      "p\n",
      "{\n",
      "margin-left:90px;\n",
      "font-size:16px;\n",
      "max-width:900px\n",
      "}\n",
      "p.blocktext {\n",
      "    margin-left: auto;\n",
      "    margin-right: auto;\n",
      "    width: 13em\n",
      "}\n",
      "h1,h2\n",
      "{\n",
      "margin-left:90px;\n",
      "color:#A8A8A8\n",
      "}\n",
      "blockquote\n",
      "{\n",
      "max-width:500px\n",
      "}\n",
      "div.cell\n",
      "{\n",
      "max-width:900px\n",
      "}\n",
      "notebook \n",
      "{ /* centre the content */\n",
      "    background: #fff;\n",
      "    color: #333;\n",
      "    width: 100ex;\n",
      "    margin: auto;\n",
      "    padding-left: 1em;\n",
      "    padding-right: 1em;\n",
      "    padding-top: 2ex;\n",
      "}\n",
      "div.output_subarea \n",
      "{\n",
      "/*background: #f5f5f5;*/\n",
      "/*border: 1px solid #ccc;*/\n",
      "/*border-radius: 4px;*/\n",
      "}\n",
      "</style>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naive Bayesian Classifier\n",
      "=============================\n",
      "By: David Rodriguez -- SFSU -- CS 869 -- April 3, 2014\n",
      "\n",
      "\n",
      "Intro\n",
      "-----------------------\n",
      "In the following document we will investigate a naive bayesian classification set-up and test the results with five and ten fold cross validation. So let's begin our tour.\n",
      "\n",
      "First up we start our doc with a few imports. For now, let's not worry about these too much. Especially since I didn't either, since I added them as I went along."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import urllib2,StringIO,csv, math, re\n",
      "import pandas as pd\n",
      "from pandas import DataFrame, Series\n",
      "import scipy.stats as ss\n",
      "import scipy as sp\n",
      "import matplotlib.mlab as mlab\n",
      "import operator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, up we need to retrieve our data. So that's what we'll do next. To run this document, you'll need to connect to the web so you can fire this next code cell off. I added an exception handler in case you get some funky responses when trying this in the future (perhaps the url will change!). The only tricky thing here I would watch for, is that the response will come back as weird text, so I used a little StringIO magic to parse it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
      "\n",
      "try: urllib2.urlopen(url)\n",
      "except URLError as e:\n",
      "  print e.reason \n",
      "\n",
      "response = urllib2.urlopen(url).read()\n",
      "output = StringIO.StringIO(response)\n",
      "cr = csv.reader(output)\n",
      "    \n",
      "data = [row for row in cr]\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having retreived our data and parsed it, we now put it in a nice data structure to live in. I'd like to comment that I originally began to code this algorithm wihtout visual aid, hence the choice of using the panda DataFrame. I am familiar with the 'cut' method and began this problem that way.... "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = DataFrame(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "don't mind this little function, it's just a little helper to take our df data to the correct data type."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cast_float(lst):\n",
      "  return [float(i) for i in lst]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... back to what I was saying. So I began this problem without visual aid. But once I did and looked around a bit, I found some pretty cool methods hidden in the matplotlib objects.\n",
      "\n",
      "For example, below is a little function that does some aesthetic cleaning to present our data. Essentially all the information to pull off naive bayesian classification is right here! Cool, right? In other words, right from our pics we can begin to unwind the parts we need to discretize our data and do some naive bayesian classification.\n",
      "\n",
      ".... but before I get too far ahead. Just take a look at this function creating a histogram for each of our variables into equal size bins."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hist_w_norm(indx,aseries,nbins,bar_color):\n",
      "\n",
      "  # best fit of data\n",
      "  (mu, sigma) = ss.norm.fit(aseries)\n",
      "\n",
      "  # the histogram of the data\n",
      "  fig = plt.figure()\n",
      "  ax = fig.add_axes([0,0,1,1])\n",
      "  ax.set_frame_on(False)\n",
      "  ax.yaxis.set_visible(False)\n",
      "  fig.set_size_inches(12,.3)\n",
      "  ax.patch.set_alpha(0.1)\n",
      "  n, bins, patches = ax.hist(cast_float(aseries), nbins, normed=1, \n",
      "                             facecolor=bar_color, alpha=0.3)\n",
      "  \n",
      "  # add a 'best fit' line\n",
      "  y = mlab.normpdf(bins, mu, sigma)\n",
      "  l = ax.plot(bins, y, color = 'black', linewidth=.3)\n",
      "  #plot\n",
      "  ax.tick_params(length=0,color='black',labelcolor='black',right='off',left='off',top='off')\n",
      "  plt.title(r'col: {x} mu: {y} sigma: {z}'.format(x=indx,y=round(mu,3),z=round(sigma,3)),\n",
      "            fontsize=8, x=-.1,y=0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below is another helper function that I created to just automate creating our plots for each variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def circulate_plots(df, start,stop,nbins,barcolor):\n",
      "  i = 'Nothing'\n",
      "  for i in range(start,stop):\n",
      "    hist_w_norm(i,cast_float(df[i]),nbins,barcolor)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ".... and of course some colors!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set_colors = ['#FF3300','#9900FF','#9900FF']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... and another helper to make various plots with different bin widths."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_plots(data_frame,start,stop,the_bins, bar_color):\n",
      "  for bin_size in the_bins: \n",
      "    circulate_plots(data_frame,start,stop,bin_size,bar_color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So our implementation of naive bayesian classification is going to classify objects into two classes. That is, we are going to do a binary classification. In our case, you are either 'M' or 'B'.  \n",
      "\n",
      "Here we are going to make some specific plots. That is we are going to plot and discretize over the variables associated with all the 'M'. I think it is pretty easy to tell that there is a lot of information in these charts. But notice, it is containing four different bin sizes over all the variables. As you scrolll down the bin sizes increase.\n",
      "\n",
      "Also! If you look closely ther is a plotted normal distribution which overlays the bars."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generate_plots(df[df[1]=='M'],2,32,[5,10,50,100],set_colors[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... and this is the variables plotted for our classifier 'B'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#generate_plots(df[df[1]=='B'],2,32,[5,10,50,100],set_colors[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice\n",
      "------------------------\n",
      "Well, that was fun. But we were a little careless (maybe too ambitious) and so we lost some information. Notice, the histogram gives us discrete probabilities and we already have a normal probability fit. So let's rewrite this plot function with these essential parts. \n",
      "\n",
      "In fact, while we are at it, let's inject with function with all the information we need. That is, we will hide our discrete probabilities. But we will also approximate a normal and guassian curve to our variables and hide the important parts of those functions for later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hist_w_norm_data(aseries,nbins):\n",
      "\n",
      "  # best fit of data assuming normal curve\n",
      "  (mu, sigma) = ss.norm.fit(aseries)\n",
      "  \n",
      "  # best fit of data assuming gamma distribution\n",
      "  fit_alpha,fit_loc,fit_beta= ss.gamma.fit(aseries)  \n",
      "    \n",
      "  #discretized intervals and probabilities\n",
      "  n, bins, patches = plt.hist(cast_float(aseries), nbins, normed=1)\n",
      "  plt.close()\n",
      "  return {'mu': mu,'sigma': sigma,'bins': patches, 'fit_alpha': fit_alpha,'fit_loc':fit_loc,'fit_beta':fit_beta}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we are just going to going through all our variables and get the data from our 'hist_w_norm_data' function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def data_for_bayes(dframe,nbins):  \n",
      "  return { i : hist_w_norm_data(cast_float(dframe[i]),nbins) for i in range(2,32)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, suppose for a second we actually were finished with our naive bayesian algorithm. Great, right? No! We need a way to verify or test the result we do get. So, we will now Create a few little functions to perform what is called cross validation. That is, we are going to call our 'hist_w_norm_data' function on only a very specific subset of our data. Then 'test' how well our algorithm classifies our 'test'. \n",
      "\n",
      "But, we have to be careful. And cross-validation is just a method that is careful. In a simple way, we will partition our data into k blocks. Create leave one block as our 'test' data and use the rest to throw at the function 'hist_w_norm_data'. Then roatate so that every block will have had a turn as 'test' data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_folding(dframe,kfolds):\n",
      "  a = dframe.index\n",
      "  bin_size =  int(len(a)/kfolds)\n",
      "  the_bins = {}\n",
      "  for i in range(kfolds-1):\n",
      "    b = np.random.choice(a, bin_size, replace=False)\n",
      "    the_bins[i] = b\n",
      "    a = a-b\n",
      "  the_bins[kfolds -1 ] = a\n",
      "  return the_bins"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ".... some more set up of the cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_folding_tests(dframe, folds):\n",
      "  return {i: {'test': folds[i],'train': dframe.index - folds[i]} for i in range(len(folds))}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... a horribly named function. But you get the idea! This function below will wrap nicely your request where all you have to specify is a data frame to use and the number of folds or blocks you want to split your data frame into."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def let_me_pick_the_test(dframe,kfolds):\n",
      "  the_combinations = k_folding_tests(dframe,k_folding(dframe,kfolds))\n",
      "  return {i :{'test': dframe.iloc[the_combinations[i]['test']],\n",
      "              'train':dframe.iloc[the_combinations[i]['train']]\n",
      "              } for i in range(kfolds)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function below will go through the k - 'folds' and picking off the training and testing blocks.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_train_m_b_probabilities(the_data,one_fold,nbins):\n",
      "  train_set = the_data[one_fold]['train']\n",
      "  test_set  = the_data[one_fold]['test']\n",
      "  train_mdat = train_set[train_set[1]=='M']\n",
      "  train_bdat = train_set[train_set[1]=='B']\n",
      "  train_mprob = data_for_bayes(train_mdat,nbins)\n",
      "  train_bprob = data_for_bayes(train_bdat,nbins)\n",
      "  return {'mprob': train_mprob,'bprob':train_bprob}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, this was a little tricky and took some digging around. And not to mention a lot of trial and error! But mission accomplished. Below we pick off the relevant information from our plot figures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_disc_probability(my_num, the_bins):\n",
      "  bin_prob = [(this.get_x(),this.get_x()+this.get_width(),this.get_height()) for this in the_bins]\n",
      "  for lala in bin_prob:\n",
      "    if  my_num >= lala[0] and my_num < lala[1]: \n",
      "      return lala[2]\n",
      "  return 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... getting all the information from the figures for each variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_row_probability(test_row,train_group):\n",
      "  probs = []\n",
      "  for i in range(2,32):\n",
      "    col = i\n",
      "    observed = float(test_row[col])\n",
      "    expected = train_group[col]['bins']\n",
      "    probs.append(find_disc_probability(observed, expected))\n",
      "  return probs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... uggh. Not just each variable. We need each row! So that's what we'll do! That's done' below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_indices_probability(test_set,train_set_probabilities):\n",
      "  probs = []\n",
      "  for i in range(len(test_set)):\n",
      "    observation = test_set.iloc[i]\n",
      "    trained     = train_set_probabilities \n",
      "    probs.append(test_row_probability(observation,trained))\n",
      "  return probs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, here's the good stuff. Suppose we have a associated probabilities for each row of our test set, supposing it was 'M' or 'B'. Essentially all we need to do is check which numerator from naive bayes is larger. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def naive_dis_class(ifm, ifb):\n",
      "  classify = []\n",
      "  for i in range(len(ifb)):\n",
      "    put_b = reduce(operator.mul, ifb[i], 1)\n",
      "    put_m = reduce(operator.mul, ifm[i], 1)\n",
      "    if put_b > put_m: \n",
      "      classify.append( 'B' )\n",
      "    elif put_b < put_m: \n",
      "      classify.append( 'M' )\n",
      "    else: \n",
      "      classify.append( 'Undecidable' )\n",
      "  return classify"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Discretized Test\n",
      "-------------------------\n",
      "So now we are read! Below are a few test that vary the bin size and the cross validation blocks. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test(the_data,nbins=5):\n",
      "  accuracy = []\n",
      "  for i in range(len(the_data)):\n",
      "    test     = the_data[i]['test']\n",
      "    trained  = get_train_m_b_probabilities(the_data,i,nbins)\n",
      "    btrained = trained['bprob']\n",
      "    mtrained = trained['mprob']\n",
      "    ifb = test_all_indices_probability(test,btrained)\n",
      "    ifm = test_all_indices_probability(test,mtrained)\n",
      "    \n",
      "    our_prediction = naive_dis_class(ifm, ifb)\n",
      "    actual         = list(test[1])\n",
      "    same = [a for a, b in zip(our_prediction, actual) if a == b]\n",
      "    \n",
      "    accuracy.append(float(len(same))/float(len(test)))\n",
      "  return accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,5)\n",
      "ans = test(the_data)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Discretization into five blocks, with 5-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.8849557522123894, 0.8053097345132744, 0.8495575221238938, 0.8495575221238938, 0.8803418803418803]\n",
        "\n",
        "--------------------------------------\n",
        "'Discretization into five blocks, with 5-fold CV\n",
        "has an accuracy of 0.853944482263 percent.\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,5)\n",
      "ans = test(the_data, nbins=10)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Discretization into ten blocks, with 5-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.8849557522123894, 0.8495575221238938, 0.8495575221238938, 0.8053097345132744, 0.7948717948717948]\n",
        "\n",
        "--------------------------------------\n",
        "'Discretization into ten blocks, with 5-fold CV\n",
        "has an accuracy of 0.836850465169 percent.\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,10)\n",
      "ans = test(the_data)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Discretization into five blocks, with 10-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.9285714285714286, 0.9107142857142857, 0.9464285714285714, 0.7857142857142857, 0.8928571428571429, 0.8035714285714286, 0.9285714285714286, 0.8392857142857143, 0.8392857142857143, 0.8307692307692308]\n",
        "\n",
        "--------------------------------------\n",
        "'Discretization into five blocks, with 10-fold CV\n",
        "has an accuracy of 0.870576923077 percent.\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,10)\n",
      "ans = test(the_data, nbins=10)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Discretization into ten blocks, with 10-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.8571428571428571, 0.875, 0.8392857142857143, 0.875, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8392857142857143, 0.7076923076923077]\n",
        "\n",
        "--------------------------------------\n",
        "'Discretization into ten blocks, with 10-fold CV\n",
        "has an accuracy of 0.842197802198 percent.\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Gaussian Test\n",
      "------------------\n",
      "So that's great. We get a pretty good estimation given our bins of size 5 and 10 based on 5 and 10 fold cross validation. Let's now assume all distributions are normal. This is a lot easier than what we just did. Notice, we need the k-folds, \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_row_normal_probability(test,trained):\n",
      "  probabilities = []\n",
      "  for i in range(2,32):\n",
      "    probabilities.append(mlab.normpdf(float(test[i]), trained[i]['mu'],trained[i]['sigma']))\n",
      "  return probabilities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_indices_normal_probability(test,trained):\n",
      "  probabilities = []\n",
      "  for i in range(len(test)):\n",
      "    probabilities.append(test_row_normal_probability(test.iloc[i], trained))\n",
      "  return probabilities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_normal(the_data, nbins=5):\n",
      "  accuracy = []\n",
      "  for i in range(len(the_data)):\n",
      "    test     = the_data[i]['test']\n",
      "    trained  = get_train_m_b_probabilities(the_data,i , nbins)\n",
      "    btrained = trained['bprob']\n",
      "    mtrained = trained['mprob']\n",
      "    ifb = test_all_indices_normal_probability(test,btrained)\n",
      "    ifm = test_all_indices_normal_probability(test,mtrained)\n",
      "\n",
      "    our_prediction = naive_dis_class(ifm, ifb)\n",
      "    actual         = list(test[1])\n",
      "    same = [a for a, b in zip(our_prediction, actual) if a == b]\n",
      "    accuracy.append(float(len(same))/float(len(test)))\n",
      "  return accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,5)\n",
      "ans = test_normal(the_data)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Assuming normality of all variables, with 5-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.9380530973451328, 0.9292035398230089, 0.9557522123893806, 0.9026548672566371, 0.9401709401709402]\n",
        "\n",
        "--------------------------------------\n",
        "'Assuming normality of all variables, with 5-fold CV\n",
        "has an accuracy of 0.933166931397 percent.\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,10)\n",
      "ans = test_normal(the_data)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Assuming normality of all variables, with 10-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.8928571428571429, 0.9285714285714286, 0.9464285714285714, 0.9107142857142857, 0.9285714285714286, 0.9464285714285714, 0.9821428571428571, 0.9285714285714286, 0.9464285714285714, 0.9230769230769231]\n",
        "\n",
        "--------------------------------------\n",
        "'Assuming normality of all variables, with 10-fold CV\n",
        "has an accuracy of 0.933379120879 percent.\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Gamma Test:\n",
      "--------------------\n",
      "\n",
      "And one thing I was curious about that was prompted way back (when we first created our plots) was whether or not our data more closely resembled gamma distributions. Gamma distributions are like Normal distributions but they have the potential to have a longer tail. In fact, it seemed a lot of our variable had some sort of lop-sided tail. Below are the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_row_gamma_probability(test,trained):\n",
      "  probabilities = []\n",
      "  for i in range(2,32):\n",
      "    probabilities.append(ss.gamma.pdf(float(test[i]), trained[i]['fit_alpha'],\n",
      "                                      loc = trained[i]['fit_loc'],\n",
      "                                      scale=trained[i]['fit_beta']))\n",
      "  return probabilities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_indices_gamma_probability(test,trained):\n",
      "  probabilities = []\n",
      "  for i in range(len(test)):\n",
      "    probabilities.append(test_row_gamma_probability(test.iloc[i], trained))\n",
      "  return probabilities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_gamma(the_data, nbins=10):\n",
      "  accuracy = []\n",
      "  for i in range(len(the_data)):\n",
      "    test     = the_data[i]['test']\n",
      "    trained  = get_train_m_b_probabilities(the_data,i,nbins)\n",
      "    btrained = trained['bprob']\n",
      "    mtrained = trained['mprob']\n",
      "    ifb = test_all_indices_gamma_probability(test,btrained)\n",
      "    ifm = test_all_indices_gamma_probability(test,mtrained)\n",
      "\n",
      "    our_prediction = naive_dis_class(ifm, ifb)\n",
      "    actual         = list(test[1])\n",
      "    same = [a for a, b in zip(our_prediction, actual) if a == b]\n",
      "    accuracy.append(float(len(same))/float(len(test)))\n",
      "  return accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,5)\n",
      "test_gamma(the_data)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Assuming a gamma distribution of all variables, with 5-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.8928571428571429, 0.9285714285714286, 0.9464285714285714, 0.9107142857142857, 0.9285714285714286, 0.9464285714285714, 0.9821428571428571, 0.9285714285714286, 0.9464285714285714, 0.9230769230769231]\n",
        "\n",
        "--------------------------------------\n",
        "'Assuming a gamma distribution of all variables, with 5-fold CV\n",
        "has an accuracy of 0.933379120879 percent.\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_data = let_me_pick_the_test(df,10)\n",
      "test_gamma(the_data)\n",
      "\n",
      "print ans\n",
      "print ''\n",
      "print '--------------------------------------'\n",
      "print ''''Assuming a gamma distribution of all variables, with 10-fold CV\n",
      "has an accuracy of {x} percent.'''.format(x = mean(ans))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.8928571428571429, 0.9285714285714286, 0.9464285714285714, 0.9107142857142857, 0.9285714285714286, 0.9464285714285714, 0.9821428571428571, 0.9285714285714286, 0.9464285714285714, 0.9230769230769231]\n",
        "\n",
        "--------------------------------------\n",
        "'Assuming a gamma distribution of all variables, with 10-fold CV\n",
        "has an accuracy of 0.933379120879 percent.\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Conclusion\n",
      "=================\n",
      "\n",
      "From the above results it appears that assuming the variables were normally distributed gives the best classification prediction. This is a bit surprising since our data sample was not very large and the use of the central limit theorem may not be warrranted. In thise case, we used approximately sample sizes of 400. This is border line,  especially if you go back and look at the plots, you will notice there is quite a of error each of the normal curves.\n",
      "\n",
      "On the other hand, it is important to contrast these results with those from the discretization of the varaibles. In particular, it appears that the blocks which were used to classify each variable were susceptible to over and under fitting of the data. Thus giving us a little less predicive power over the test set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}